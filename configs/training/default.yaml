optimizer_name: AdamW
optimizer_args:
  lr: 3e-4
  betas: [0.9, 0.98]
  fused: False  # this can only work with CUDA

scheduler_name: ExponentialLR
scheduler_args:
  gamma: 0.95

batch_size: 32
epochs: 1
dtype: 'float16'
distributed_strategy: 'ddp'
clip_grad_norm: 1.0
use_grad_scaler: True  # need to mkae sure we can only do this with float16 and cuda 