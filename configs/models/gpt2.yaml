common:
  hidden_size: 768
  context_size: 128
  vocab_size: 50304
  num_layers: 6
  weight_tying: False

attention:
  num_heads_q: 8
  num_heads_k: 8
  num_heads_v: 8
  mask: 'causal'
  is_causal: True
  attn_drop: 0.1
  output_drop: 0.1

# figure out how to add additional args to all of these
positional_encoding:
  pe_type: 'rope'

feed_forward:
  ffn_size: 2048
  activation_func: 'swiglu'
  dropout: 0.1

norm:
  norm_type: 'layer'

transformer_block:
  - norm
  - attention
  - skip
  - norm
  - feed_forward
  - skip
