common:
  hidden_size: 768
  context_size: 128
  vocab_size: 50304
  num_layers: 6
  weight_tying: True

attention:
  num_heads_q: 8
  num_heads_k: 8
  num_heads_v: 8
  mask: 'causal'
  is_causal: True
  attn_drop: 0.0
  output_drop: 0.0

# figure out how to add additional args to all of these 
positional_encoding:
  pe_type: 'sinusoidal'

feed_forward:
  ffn_size: 2048
  activation_func: 'gelu'
  dropout: 0.1

norm: 
  norm_type: 'layer'

transformer_block:
  - norm 
  - attention 
  - skip 
  - norm
  - feed_forward
  - skip 



